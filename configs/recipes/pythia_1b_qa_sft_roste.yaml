# ModelArguments
w_bits: 4
a_bits: 4
kv_bits: 4
w_sym: False
a_sym: False
kv_sym: False
w_clip_ratio: 0.95
a_clip_ratio: 0.95
kv_clip_ratio: 0.95

# SFTScriptArguments
dataset_name: trl-lib/tldr 

# SFTConfig
max_seq_length: 2048 

# ModelConfig
model_name_or_path: save/pythia-1b/ckpt/pythia-1b-deduped-new-r1

# TrainingArguments
output_dir: save/pythia-1b/ckpt/pythia-1b-deduped-new-qa-sft-4-4-4-r-123
overwrite_output_dir: True 
do_train: True 
do_eval: True 
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 1 
learning_rate: 3e-5
weight_decay: 0. 
num_train_epochs: 1 
max_steps: -1 
warmup_ratio: 0. 
lr_scheduler_type: "cosine" 
logging_steps: 1 
save_strategy: "no" 
save_safetensors: False 
seed: 42 
bf16: True 
fp16: False 
run_name: w4-a4-kv4-roste
report_to: wandb
