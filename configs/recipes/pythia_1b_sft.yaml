# SFTScriptArguments
dataset_name: trl-lib/tldr 

# SFTConfig
max_seq_length: 2048 

# ModelConfig
model_name_or_path: save/pythia-1b/ckpt/pythia-1b-deduped-new

# TrainingArguments
output_dir: save/pythia-1b/ckpt/pythia-1b-deduped-new-sft
overwrite_output_dir: True 
do_train: True 
do_eval: True 
per_device_train_batch_size: 16 
per_device_eval_batch_size: 16 
gradient_accumulation_steps: 1 
learning_rate: 3e-5 
weight_decay: 0. 
num_train_epochs: 1 
max_steps: -1 
warmup_ratio: 0. 
lr_scheduler_type: "cosine" 
logging_steps: 1 
save_strategy: "no" 
save_safetensors: False 
seed: 42 
bf16: True 
fp16: False
run_name: pythia-1b-deduped-sft
report_to: wandb