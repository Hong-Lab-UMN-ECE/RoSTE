# ModelArguments
w_bits: 4
a_bits: 4
kv_bits: 4

# SFTScriptArguments
dataset_name: trl-lib/tldr 

# SFTConfig
max_seq_length: 2048 

# ModelConfig
model_name_or_path: save/pythia-1b/ckpt/pythia-1b-deduped-new

# TrainingArguments
output_dir: save/pythia-1b/ckpt/pythia-1b-deduped-new-qa-sft-4-4-4
overwrite_output_dir: True 
do_train: True 
do_eval: True 
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 1 
learning_rate: 3e-5
weight_decay: 0. 
num_train_epochs: 1 
max_steps: -1 
warmup_ratio: 0. 
lr_scheduler_type: "cosine" 
logging_steps: 1 
save_strategy: "no" 
# save_strategy: "steps"
# save_steps: 100
save_safetensors: False 
seed: 42 
bf16: True 
fp16: False 
run_name: w4-a4-kv4-ste
report_to: wandb
